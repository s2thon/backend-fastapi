# Gerekli kÃ¼tÃ¼phaneleri ve modÃ¼lleri iÃ§e aktarÄ±yoruz
import os
import operator
from typing import TypedDict, Annotated, Literal
from dotenv import load_dotenv

# LangChain'in temel bileÅŸenleri
from langchain_core.messages import BaseMessage, HumanMessage, ToolMessage, SystemMessage
from langchain_core.tools import tool

# Google Generative AI entegrasyonlarÄ±
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings

# LangGraph'Ä±n temel yapÄ± taÅŸlarÄ±
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode # YENÄ° IMPORT

# VektÃ¶r veritabanÄ± iÃ§in gerekli LangChain community bileÅŸenleri
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import CharacterTextSplitter

# Kendi yazdÄ±ÄŸÄ±mÄ±z veritabanÄ± fonksiyonlarÄ± (araÃ§ olacaklar)
from services.supabase_client import (
    get_price_info,
    get_stock_info,
    get_payment_amount,
    get_item_status,
    get_refund_status,
)

# --- 1. UYGULAMA BAÅLANGIÃ‡ KONFÄ°GÃœRASYONU ---

# .env dosyasÄ±ndaki ortam deÄŸiÅŸkenlerini yÃ¼kle
load_dotenv()

# Embedding modelini (metinleri vektÃ¶re Ã§eviren model) yÃ¼kle
embedding_model = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001",
    google_api_key=os.getenv("GEMINI_API_KEY")
)

# VektÃ¶r veritabanÄ±nÄ±n diskteki yolunu belirt
# Projenizin kÃ¶k dizinine gÃ¶re bu yolu ayarlamanÄ±z gerekebilir.
vector_store_path = "embeddings/vector_store"

# VektÃ¶r veritabanÄ±nÄ± yÃ¼kle veya eÄŸer mevcut deÄŸilse oluÅŸtur
try:
    if not os.path.exists(os.path.join(vector_store_path, "index.faiss")):
        print("VektÃ¶r veritabanÄ± bulunamadÄ±, sÄ±fÄ±rdan oluÅŸturuluyor...")
        
        # Bilgi alÄ±nacak dokÃ¼manlarÄ±n yollarÄ±
        file_paths = ["data/documents/faq.txt", "data/documents/policy.txt"]
        docs = []
        for path in file_paths:
            loader = TextLoader(path, encoding="utf-8")
            docs.extend(loader.load())
        
        # DokÃ¼manlarÄ± daha kÃ¼Ã§Ã¼k parÃ§alara (chunk) ayÄ±r
        splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        chunks = splitter.split_documents(docs)
        
        # ParÃ§alardan FAISS vektÃ¶r veritabanÄ±nÄ± oluÅŸtur ve diske kaydet
        db = FAISS.from_documents(chunks, embedding_model)
        db.save_local(vector_store_path)
        print("âœ… VektÃ¶r veritabanÄ± baÅŸarÄ±yla oluÅŸturuldu ve kaydedildi.")
    else:
        print("Mevcut vektÃ¶r veritabanÄ± yÃ¼kleniyor...")
        db = FAISS.load_local(
            vector_store_path, 
            embedding_model, 
            allow_dangerous_deserialization=True # GÃ¼venli bir ortamda Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±nÄ±zdan emin olun
        )
        print("âœ… VektÃ¶r veritabanÄ± baÅŸarÄ±yla yÃ¼klendi.")
except Exception as e:
    print(f"âŒ VektÃ¶r veritabanÄ± yÃ¼klenirken kritik bir hata oluÅŸtu: {e}")
    db = None # Hata durumunda db'yi None yap

# --- 2. SÄ°STEM TALÄ°MATI VE ARAÃ‡ (TOOL) TANIMLAMALARI ---

# Modele kimliÄŸini ve davranÄ±ÅŸ kurallarÄ±nÄ± Ã¶ÄŸreten sistem talimatÄ±
SYSTEM_INSTRUCTION = """
### KÄ°MLÄ°K VE GÃ–REV TANIMI ###
Sen, bir e-ticaret platformunun yardÄ±msever ve profesyonel mÃ¼ÅŸteri hizmetleri asistanÄ±sÄ±n. GÃ¶revin, kullanÄ±cÄ±lardan gelen sorularÄ± sana verilen araÃ§larÄ± kullanarak doÄŸru bir ÅŸekilde yanÄ±tlamaktÄ±r.

### DAVRANIÅ KURALLARI ###
1.  **Profesyonel Dil:** CevaplarÄ±n daima resmi, net, kibar ve kurumsal bir dilde olmalÄ±dÄ±r.
2.  **AraÃ§ KullanÄ±m Ã–nceliÄŸi:** KullanÄ±cÄ± bir Ã¼rÃ¼nÃ¼n fiyatÄ±nÄ±, stokunu, sipariÅŸ durumunu, iade detayÄ±nÄ± veya iade/kargo gibi genel politikalarÄ± sorduÄŸunda, bu bilgileri tahmin etmek yerine sana verilen araÃ§larÄ± ('tools') kullanmak zorundasÄ±n. EÄŸer soruya uygun bir araÃ§ varsa, mutlaka kullan.
3.  **Net ve OdaklÄ± Cevap:** Sadece sorulan soruya odaklan. AraÃ§lardan gelen bilgiyi temiz ve anlaÅŸÄ±lÄ±r bir ÅŸekilde kullanÄ±cÄ±ya sun.
"""

# FonksiyonlarÄ± LangGraph'Ä±n anlayacaÄŸÄ± 'araÃ§lara' dÃ¶nÃ¼ÅŸtÃ¼rÃ¼yoruz.
# Her aracÄ±n docstring'i, modelin o aracÄ± ne zaman kullanacaÄŸÄ±nÄ± anlamasÄ± iÃ§in kritiktir.

@tool
def get_price_info_tool(product_name: str) -> str:
    """Bir Ã¼rÃ¼nÃ¼n fiyatÄ±nÄ± Ã¶ÄŸrenmek iÃ§in kullanÄ±lÄ±r. ÃœrÃ¼nÃ¼n adÄ±nÄ± parametre olarak alÄ±r."""
    return get_price_info(product_name)

@tool
def get_stock_info_tool(product_name: str) -> str:
    """Bir Ã¼rÃ¼nÃ¼n stokta kaÃ§ adet olduÄŸunu Ã¶ÄŸrenmek iÃ§in kullanÄ±lÄ±r. ÃœrÃ¼nÃ¼n adÄ±nÄ± parametre olarak alÄ±r."""
    return get_stock_info(product_name)

@tool
def get_payment_amount_tool(order_id: int) -> str:
    """Bir sipariÅŸin toplam Ã¶deme tutarÄ±nÄ± Ã¶ÄŸrenmek iÃ§in kullanÄ±lÄ±r. SipariÅŸ ID'sini parametre olarak alÄ±r."""
    return get_payment_amount(order_id)

@tool
def get_item_status_tool(order_id: int, product_name: str) -> str:
    """Belirli bir sipariÅŸteki bir Ã¼rÃ¼nÃ¼n durumunu (kargolandÄ±, hazÄ±rlanÄ±yor vb.) Ã¶ÄŸrenmek iÃ§in kullanÄ±lÄ±r. SipariÅŸ ID'si ve Ã¼rÃ¼n adÄ± gereklidir."""
    return get_item_status(order_id, product_name)

@tool
def get_refund_status_tool(order_id: int, product_name: str) -> str:
    """Belirli bir sipariÅŸteki bir Ã¼rÃ¼nÃ¼n iade durumunu (onaylandÄ±, bekleniyor vb.) Ã¶ÄŸrenmek iÃ§in kullanÄ±lÄ±r. SipariÅŸ ID'si ve Ã¼rÃ¼n adÄ± gereklidir."""
    return get_refund_status(order_id, product_name)

@tool
def search_documents_tool(query: str) -> str:
    """
    KullanÄ±cÄ±nÄ±n iade politikasÄ±, kargo sÃ¼reci, ÅŸirket hakkÄ±ndaki genel bilgiler, 
    kullanÄ±m koÅŸullarÄ± veya sÄ±kÃ§a sorulan sorular (SSS) gibi genel bir sorusu olduÄŸunda kullanÄ±lÄ±r.
    ÃœrÃ¼n fiyatÄ±, stok durumu, sipariÅŸ durumu veya iade durumu gibi spesifik veritabanÄ± bilgileri iÃ§in KULLANILMAZ.
    Sadece genel ve politikaya dayalÄ± sorular iÃ§in bu aracÄ± kullan.
    """
    if not db:
        return "Belge veritabanÄ± ÅŸu anda kullanÄ±lamÄ±yor."
    
    print(f"ğŸ“„ Belge aramasÄ± (RAG) yapÄ±lÄ±yor: '{query}'")
    docs = db.similarity_search(query, k=3)
    
    if not docs:
        return "Belgelerde bu konuyla ilgili bir bilgi bulunamadÄ±."
        
    context = "\n\n---\n\n".join(doc.page_content for doc in docs)
    return f"Konuyla ilgili belgelerden ÅŸu bilgiler bulundu:\n\n{context}"


# TÃ¼m araÃ§larÄ± bir listeye ve bu listeyi Ã§alÄ±ÅŸtÄ±racak bir ToolExecutor'a ekleyelim
tools = [
    get_price_info_tool, 
    get_stock_info_tool, 
    get_payment_amount_tool, 
    get_item_status_tool, 
    get_refund_status_tool,
    search_documents_tool, # Yeni RAG aracÄ±mÄ±zÄ± da listeye ekledik
]

# --- 3. MODEL VE GRAFÄ°K (LANGGRAPH) YAPILANDIRMASI ---

# Modeli streaming (akÄ±ÅŸ) ve araÃ§ kullanÄ±mÄ± iÃ§in yapÄ±landÄ±r
model = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash", 
    temperature=0.1, 
    google_api_key=os.getenv("GEMINI_API_KEY") # API AnahtarÄ±nÄ± doÄŸrudan veriyoruz
)
model_with_tools = model.bind_tools(tools)

# GrafiÄŸimizin durumunu (hafÄ±zasÄ±nÄ±) tanÄ±mlayan yapÄ±
class GraphState(TypedDict):
    messages: Annotated[list[BaseMessage], operator.add]

# GrafiÄŸin karar verme ve iÅŸlem yapma dÃ¼ÄŸÃ¼mleri (node'lar)
def should_continue(state: GraphState) -> Literal["tools", "end"]:
    """Modelin son Ã§Ä±ktÄ±sÄ±nÄ± analiz eder: AraÃ§ mÄ± Ã§aÄŸÄ±racak, yoksa konuÅŸma bitti mi?"""
    if state["messages"][-1].tool_calls:
        return "tools"
    return "end"

def call_model(state: GraphState):
    """LLM'i (yapay zeka modelini) Ã§aÄŸÄ±ran ana dÃ¼ÄŸÃ¼m."""
    response = model_with_tools.invoke(state["messages"])
    return {"messages": [response]}


# --- 4. GRAFÄ°ÄÄ° OLUÅTURMA VE DERLEME ---

workflow = StateGraph(GraphState)

# DÃ¼ÄŸÃ¼mleri (node) grafiÄŸe ekle
workflow.add_node("agent", call_model)
# --- DEÄÄ°ÅÄ°KLÄ°K: Manuel call_tools fonksiyonu yerine hazÄ±r ToolNode kullanÄ±yoruz ---
# Bu, kodu daha temiz ve kÃ¼tÃ¼phane gÃ¼ncellemelerine daha dayanÄ±klÄ± hale getirir.
workflow.add_node("tools", ToolNode(tools))

# GrafiÄŸin baÅŸlangÄ±Ã§ noktasÄ±nÄ± belirle
workflow.set_entry_point("agent")

# KenarlarÄ± (edge) ve karar mekanizmasÄ±nÄ± tanÄ±mla
workflow.add_conditional_edges(
    "agent",
    should_continue,
    {
        "tools": "tools",
        "end": END,
    },
)
workflow.add_edge("tools", "agent")

# GrafiÄŸi Ã§alÄ±ÅŸtÄ±rÄ±labilir bir uygulama haline getir
langgraph_app = workflow.compile()

# --- 5. ASENKRON Ã‡ALIÅTIRMA FONKSÄ°YONU ---

async def run_langgraph_chat_async(user_input: str):
    """
    LangGraph uygulamasÄ±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±r ve yanÄ±tlarÄ± FastAPI iÃ§in akÄ±ÅŸ halinde dÃ¶ndÃ¼rÃ¼r.
    Sohbeti sistem talimatÄ± ile baÅŸlatÄ±r.
    """
    inputs = {
        "messages": [
            SystemMessage(content=SYSTEM_INSTRUCTION),
            HumanMessage(content=user_input)
        ]
    }
    
    # .astream() metodu, yanÄ±tÄ±n adÄ±mlarÄ±nÄ± asenkron bir akÄ±ÅŸ olarak almanÄ±zÄ± saÄŸlar
    async for output in langgraph_app.astream(inputs):
        # AkÄ±ÅŸtan gelen her adÄ±mdaki son mesajÄ± kontrol et
        if "agent" in output:
            last_message = output["agent"]["messages"][-1]
            # Sadece modelin son, nihai cevabÄ±nÄ± (araÃ§ Ã§aÄŸÄ±rmadÄ±ÄŸÄ± zaman) kullanÄ±cÄ±ya gÃ¶nder
            if not last_message.tool_calls and last_message.content:
                yield last_message.content