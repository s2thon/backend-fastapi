from langchain_core.messages import AIMessage, HumanMessage
# DÄ°KKAT: `check_cache` iÃ§inde oluÅŸturulan aynÄ± cache_manager nesnesini ve yardÄ±mcÄ± fonksiyonlarÄ± kullanÄ±yoruz.
from .check_cache import cache_manager, generate_query_hash
from ..graph_state import GraphState

def cache_final_answer(state: GraphState) -> dict:
    """GrafiÄŸin sonunda, nihai AI yanÄ±tÄ±nÄ± Ã¶nbelleÄŸe alÄ±r."""
    # ... (cache_final_answer fonksiyonunun tÃ¼m iÃ§eriÄŸi buraya, deÄŸiÅŸiklik yok) ...
    tool_calls = []
    for message in reversed(state["messages"]):
        if isinstance(message, AIMessage) and message.tool_calls:
            tool_calls = message.tool_calls
            break
    called_tool_names = {call['name'] for call in tool_calls}
    if 'get_stock_info_tool' in called_tool_names: # VarsayÄ±msal stok aracÄ±
        print("â„¹ï¸ YanÄ±t, anlÄ±k bilgi iÃ§erdiÄŸi iÃ§in Ã¶nbelleÄŸe alÄ±nmayacak.")
        return {}
        
    last_message = state["messages"][-1]
    if isinstance(last_message, AIMessage) and last_message.content:
        # HatalÄ±/olumsuz yanÄ±tlarÄ± Ã¶nbelleÄŸe alma
        error_keywords = ["sorun", "hata", "bulunamadÄ±", "Ã¼zgÃ¼nÃ¼m"]
        if any(keyword in last_message.content.lower() for keyword in error_keywords):
            print(f"ğŸš« HatalÄ±/olumsuz yanÄ±t Ã¶nbelleÄŸe alÄ±nmayacak.")
            return {}

        user_messages = [msg for msg in state["messages"] if isinstance(msg, HumanMessage)]
        if user_messages:
            last_user_query = user_messages[-1].content
            query_hash = generate_query_hash(last_user_query)
            cache_manager.set(query_hash, last_message.content)
            print(f"ğŸ’¾ Ã–nbelleÄŸe eklendi: '{last_user_query}'")
    return {}