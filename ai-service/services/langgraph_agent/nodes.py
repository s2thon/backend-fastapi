# Bu dosya, grafiÄŸin mantÄ±ÄŸÄ±nÄ± iÃ§eren dÃ¼ÄŸÃ¼m fonksiyonlarÄ±nÄ± (call_model, should_continue) barÄ±ndÄ±rÄ±r.

import json
from typing import Literal, Dict, Any
import time
import re
import hashlib
import os
from typing import Literal, Dict, Any
from threading import Lock

from .graph_state import GraphState
from langchain_core.messages import ToolMessage, SystemMessage, AIMessage, HumanMessage

class PersistentCacheManager:
    # 1 gÃ¼nlÃ¼kj cache sÃ¼resi (TTL) ve maksimum Ã¶nbellek boyutu
    # Bu deÄŸerler, uygulama baÅŸlatÄ±ldÄ±ÄŸÄ±nda ayarlanÄ±r ve deÄŸiÅŸtirilebilir.
    # TTL: 86400 saniye (1 gÃ¼n)
    # max_size: 100 Ã¶ÄŸe
    def __init__(self, cache_file='faq_cache.json', ttl=86400, max_size=100):
        self.cache_file = cache_file
        self.ttl = ttl
        self.max_size = max_size
        self._cache = self._load_cache()
        self._lock = Lock()  # Dosya iÅŸlemlerini thread-safe yapmak iÃ§in

    def _load_cache(self) -> dict:
        """JSON dosyasÄ±ndan Ã¶nbelleÄŸi yÃ¼kler."""
        if not os.path.exists(self.cache_file):
            return {}
        try:
            with open(self.cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError):
            # Dosya bozuksa veya okunamÄ±yorsa boÅŸ bir Ã¶nbellekle baÅŸla
            return {}

    def _save_cache(self):
        """Ã–nbelleÄŸi JSON dosyasÄ±na kaydeder."""
        with self._lock:
            try:
                with open(self.cache_file, 'w', encoding='utf-8') as f:
                    json.dump(self._cache, f, indent=4)
            except IOError as e:
                print(f"âŒ Ã–nbellek dosyasÄ± yazÄ±lamadÄ±: {e}")

    def get(self, key: str) -> Any | None:
        """Ã–nbellekten bir deÄŸer alÄ±r ve TTL kontrolÃ¼ yapar."""
        if key not in self._cache:
            return None
        
        entry = self._cache[key]
        cached_response, timestamp = entry['response'], entry['timestamp']
        
        # TTL kontrolÃ¼ - Ã¶nbellek sÃ¼resi dolmuÅŸ mu?
        if time.time() - timestamp > self.ttl:
            print(f"â³ Ã–nbellek sÃ¼resi doldu: '{key}'")
            del self._cache[key]
            self._save_cache()
            return None
            
        return cached_response

    def set(self, key: str, value: Any):
        """Ã–nbelleÄŸe yeni bir deÄŸer ekler ve boyut kontrolÃ¼ yapar."""
        # Ã–nbellek boyut kontrolÃ¼
        if len(self._cache) >= self.max_size and key not in self._cache:
            # En eski girdiyi bul ve sil
            oldest_key = min(self._cache.keys(), key=lambda k: self._cache[k]['timestamp'])
            del self._cache[oldest_key]
        
        self._cache[key] = {
            "response": value,
            "timestamp": time.time()
        }
        self._save_cache()


# Bu nesne, uygulama Ã§alÄ±ÅŸtÄ±ÄŸÄ± sÃ¼rece bir kez oluÅŸturulur ve kullanÄ±lÄ±r.
cache_manager = PersistentCacheManager()

def normalize_query(query: str) -> str:
    """
    KullanÄ±cÄ± sorgusunu Ã¶nbellek aramasÄ± iÃ§in normalleÅŸtir.
    Noktalama iÅŸaretlerini kaldÄ±r, lowercase yap.
    """
    # Noktalama iÅŸaretlerini kaldÄ±r
    normalized = re.sub(r'[^\w\s]', '', query.lower())
    # Gereksiz boÅŸluklarÄ± temizle
    normalized = " ".join(normalized.split())
    return normalized

def generate_query_hash(query: str) -> str:
    """NormalleÅŸtirilmiÅŸ sorgudan benzersiz bir hash oluÅŸtur."""
    normalized = normalize_query(query)
    return hashlib.md5(normalized.encode('utf-8')).hexdigest()

def check_cache(state: GraphState) -> dict:
    """
    SÄ±k sorulan sorular iÃ§in Ã¶nbellek kontrolÃ¼.
    EÄŸer soru daha Ã¶nce sorulmuÅŸ ve cevabÄ± Ã¶nbellekte varsa,
    grafiÄŸi LLM'e yÃ¶nlendirmeden hemen cevabÄ± dÃ¶ndÃ¼rÃ¼r.
    
    Ã–nbellek hit olursa bÃ¼yÃ¼k bir performans ve maliyet kazancÄ± saÄŸlar.
    """
    # Son kullanÄ±cÄ± mesajÄ±nÄ± al
    last_message = next((msg for msg in reversed(state["messages"]) if hasattr(msg, 'content')), None)
    if not last_message or not last_message.content or len(last_message.content) < 5:
        return {}
    
    query = last_message.content
    query_hash = generate_query_hash(query)
    
    cached_response = cache_manager.get(query_hash)
    
    if cached_response:
        print(f"ğŸ¯ Ã–nbellek HIT: '{query}'")
        return {
            "messages": [AIMessage(content=f"{cached_response}")],
            "cached": True
        }
    
    print(f"â“ Ã–nbellek MISS: '{query}'")
    return {}

# YENÄ°: Ã–nbelleÄŸe yazma iÅŸini yapan Ã¶zel bir dÃ¼ÄŸÃ¼m
def cache_final_answer(state: GraphState) -> dict:
    """
    GrafiÄŸin sonunda, nihai AI yanÄ±tÄ±nÄ± Ã¶nbelleÄŸe alÄ±r.
    """

    # 1. CevabÄ± oluÅŸturmak iÃ§in hangi araÃ§larÄ±n kullanÄ±ldÄ±ÄŸÄ±nÄ± bul.
    # Sohbet geÃ§miÅŸinde geriye doÄŸru giderek tool_calls iÃ§eren son AIMessage'Ä± bul.
    tool_calls = []
    for message in reversed(state["messages"]):
        if isinstance(message, AIMessage) and message.tool_calls:
            tool_calls = message.tool_calls
            break
    
    # 2. KullanÄ±lan araÃ§larÄ±n isimlerini bir listeye al.
    called_tool_names = {call['name'] for call in tool_calls}
    
    # 3. EÄŸer stok sorgulama aracÄ± kullanÄ±ldÄ±ysa, Ã¶nbelleÄŸe ALMA.
    if 'get_stock_info_tool' in called_tool_names:
        print("â„¹ï¸ YanÄ±t, anlÄ±k stok bilgisi iÃ§erdiÄŸi iÃ§in Ã¶nbelleÄŸe alÄ±nmayacak.")
        return {} # Fonksiyonu burada sonlandÄ±r.
        
    # 4. Stok aracÄ± kullanÄ±lmadÄ±ysa, normal Ã¶nbelleÄŸe alma iÅŸlemini yap.
    last_message = state["messages"][-1]


    final_content = last_message.content

    # Hata veya olumsuz sonuÃ§ belirten anahtar kelimeleri tanÄ±mla
    # Bu liste, Ã¶nbelleÄŸe alÄ±nmasÄ±nÄ± istemediÄŸimiz durumlarÄ± kapsar.
    error_keywords = [
        "sorun yaÅŸandÄ±",
        "hata oluÅŸtu",
        "bulunamadÄ±",
        "baÅŸvurun",  # "sistem yÃ¶neticisine baÅŸvurun" gibi ifadeler iÃ§in
        "Ã¼zgÃ¼nÃ¼m"   # "ÃœzgÃ¼nÃ¼m, ... gibi ifadeler iÃ§in"
    ]


    if isinstance(last_message, AIMessage) and last_message.content:
        if any(keyword in final_content.lower() for keyword in error_keywords):
            # Ekrana log basarken yanÄ±tÄ±n sadece bir kÄ±smÄ±nÄ± gÃ¶stererek terminali temiz tutalÄ±m.
            print(f"ğŸš« HatalÄ±/olumsuz yanÄ±t Ã¶nbelleÄŸe alÄ±nmayacak: '{final_content[:70]}...'")
            return {}  # Ã–nbelleÄŸe almadan fonksiyonu sonlandÄ±r

        # 4. YanÄ±t temizse, normal Ã¶nbelleÄŸe alma iÅŸlemini yap
        # ArtÄ±k stok kontrolÃ¼ gibi eski mantÄ±klara gerek yok.
        user_messages = [msg for msg in state["messages"] if isinstance(msg, HumanMessage)]
        if user_messages:
            last_user_query = user_messages[-1].content
            query_hash = generate_query_hash(last_user_query)
            cache_manager.set(query_hash, final_content)
            print(f"ğŸ’¾ Ã–nbelleÄŸe eklendi: '{last_user_query}'")
                
    return {}


def summarize_tool_outputs(state: GraphState):
    """
    AraÃ§lardan gelen Ã§Ä±ktÄ±larÄ± akÄ±llÄ±ca birleÅŸtirir.
    - BaÅŸarÄ±lÄ± sonuÃ§larÄ± listeler.
    - "BulunamadÄ±" veya "tÃ¼kendi" gibi bilgilendirici ama "baÅŸarÄ±sÄ±z" olmayan sonuÃ§larÄ± doÄŸru ÅŸekilde ekler.
    - Tavsiye aracÄ±ndan gelen boÅŸ sonuÃ§larÄ± tamamen gÃ¶rmezden gelerek sessiz kalmasÄ±nÄ± saÄŸlar.
    - GerÃ§ek veritabanÄ± hatalarÄ±nÄ± belirtir.
    """
    messages = state["messages"]
    
    # Sadece bu dÃ¶ngÃ¼yle ilgili ToolMessage'larÄ± al
    last_agent_message = next((msg for msg in reversed(messages) if hasattr(msg, 'tool_calls') and msg.tool_calls), None)
    if not last_agent_message:
        return {}
    tool_call_ids = {tc['id'] for tc in last_agent_message.tool_calls}
    tool_outputs = [msg for msg in messages if isinstance(msg, ToolMessage) and msg.tool_call_id in tool_call_ids]
    
    if not tool_outputs:
        return {}

    summary_lines = []
    for output in tool_outputs:
        content = output.content.strip() if output.content else ""
        tool_name = output.name

        # 1. Tavsiye aracÄ±ndan gelen boÅŸ cevabÄ± tamamen yoksay.
        if tool_name == 'get_recommendations_tool' and not content:
            continue  # Bu Ã§Ä±ktÄ±yÄ± Ã¶zete hiÃ§ ekleme.

        # 2. GerÃ§ek bir veritabanÄ± hatasÄ± varsa belirt.
        if "hatasÄ± oluÅŸtu" in content:
            summary_lines.append(f"- {tool_name} kullanÄ±lÄ±rken bir sorun yaÅŸandÄ±.")
        # 3. AnlamlÄ± bir sonuÃ§ varsa (boÅŸ deÄŸilse) ekle.
        #    Bu, "stok tÃ¼kendi" veya "Ã¼rÃ¼n bulunamadÄ±" gibi geÃ§erli bilgileri de kapsar.
        elif content:
            summary_lines.append(f"- {content}")

    # Ã–zetlenecek anlamlÄ± bir bilgi yoksa, sonraki adÄ±mÄ± karÄ±ÅŸtÄ±rmamak iÃ§in boÅŸ dÃ¶n.
    if not summary_lines:
        return {}

    summary = "AraÃ§lardan ÅŸu bilgiler toplandÄ±:\n" + "\n".join(summary_lines)
    
    print(f"ğŸ“‹ AkÄ±llÄ± Ã–zetleme TamamlandÄ±:\n{summary}")

    return {"messages": [SystemMessage(content=summary)]}


def call_model(state: GraphState, model_with_tools):
    """
    LLM'i (yapay zeka modelini) Ã§aÄŸÄ±ran ana dÃ¼ÄŸÃ¼m.
    Modeli dÄ±ÅŸarÄ±dan parametre olarak alarak daha esnek bir yapÄ± sunar.
    """
    # Mevcut sohbet geÃ§miÅŸini al
    messages = state["messages"]
    # Modeli bu geÃ§miÅŸle Ã§aÄŸÄ±r ve yanÄ±tÄ±nÄ± al
    response = model_with_tools.invoke(messages)
    # Gelen yanÄ±tÄ± mesaj listesine eklenmek Ã¼zere dÃ¶ndÃ¼r
    return {"messages": [response]}

def validate_input(state: GraphState) -> dict:
    """
    KullanÄ±cÄ± girdisini doÄŸrular ve temizler.
    ZararlÄ± iÃ§erik, Ã§ok uzun mesajlar veya boÅŸ girdileri kontrol eder.
    """
    last_message = state["messages"][-1]
    content = last_message.content
    
    # BoÅŸ girdi kontrolÃ¼
    if not content or content.strip() == "":
        return {
            "messages": [
                {"role": "assistant", "content": "LÃ¼tfen bir soru veya mesaj yazÄ±n."}
            ],
            "validation_error": True
        }
    
    # Ã‡ok uzun mesaj kontrolÃ¼
    if len(content) > 1000:
        return {
            "messages": [
                {"role": "assistant", "content": "MesajÄ±nÄ±z Ã§ok uzun. LÃ¼tfen daha kÄ±sa bir mesaj gÃ¶nderin."}
            ],
            "validation_error": True
        }
    
    # ZararlÄ± iÃ§erik kontrolÃ¼ (geliÅŸtirilmiÅŸ)
    harmful_keywords = ["hack", "spam", "virus", "malware", "phishing", "scam"]
    if any(keyword in content.lower() for keyword in harmful_keywords):
        return {
            "messages": [
                {"role": "assistant", "content": "Bu tÃ¼r iÃ§erikler iÃ§in yardÄ±m saÄŸlayamam. LÃ¼tfen uygun bir soru sorun."}
            ],
            "validation_error": True
        }
    
    # Rate limiting kontrolÃ¼ (IP bazlÄ± veya session bazlÄ±)
    if len(content.split()) > 200:  # Ã‡ok fazla kelime
        return {
            "messages": [
                {"role": "assistant", "content": "LÃ¼tfen daha kÄ±sa ve Ã¶z bir mesaj gÃ¶nderin."}
            ],
            "validation_error": True
        }
    
    # BaÅŸarÄ±lÄ± validasyon
    return {
        "validated": True,
        "validation_error": False
    }

def enhanced_should_continue(state: GraphState) -> Literal["tools", "error", "cache_and_end"]:
    """
    BasitleÅŸtirilmiÅŸ karar verme mantÄ±ÄŸÄ± - sadece tools, error, veya end.
    """
    if state.get("cached") or state.get("validation_error"):
        return "cache_and_end"
    
    last_message = state["messages"][-1]

    if state.get("error"):
        return "error"
    
    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
        return "tools"
    
    return "cache_and_end"




