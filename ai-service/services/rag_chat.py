# RAG pipeline (chatbot)

import os
from dotenv import load_dotenv
import google.generativeai as genai

from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_text_splitters import CharacterTextSplitter

import google.generativeai as genai

# .env dosyasÄ±nÄ± yÃ¼kle
load_dotenv()

# Gemini API yapÄ±landÄ±rmasÄ±
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
llm = genai.GenerativeModel("gemini-1.5-flash")  # veya "gemini-1.5-flash"

# Embedding nesnesi
embedding = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001",
    google_api_key=os.getenv("GEMINI_API_KEY")
)

# DÃ¶kÃ¼manlardan vektÃ¶r veritabanÄ± oluÅŸtur (tek seferlik)
def build_vector_store():
    # Ã‡oklu dosyalarÄ± buraya ekleyebilirsin
    file_paths = [
        "data/documents/faq.txt",
        "data/documents/policy.txt",
    ]

    docs = []
    for path in file_paths:
        loader = TextLoader(path, encoding="utf-8")
        docs.extend(loader.load())

    splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = splitter.split_documents(docs)

    vectorstore = FAISS.from_documents(chunks, embedding)
    vectorstore.save_local("embeddings/vector_store")


# VektÃ¶r veritabanÄ±nÄ± yÃ¼kle
db = FAISS.load_local("embeddings/vector_store", embedding, allow_dangerous_deserialization=True)


# Sorgu iÃ§in RAG Chatbot (RAG logic burada)
def rag_chat(user_input: str) -> str:

    # KullanÄ±cÄ± mesajÄ±na en yakÄ±n dÃ¶kÃ¼man parÃ§alarÄ±nÄ± al
    docs = db.similarity_search(user_input, k=3)
    context = "\n\n".join(doc.page_content for doc in docs)

    # Prompt oluÅŸtur
    prompt = f"""
AÅŸaÄŸÄ±daki bilgileri kullanarak kullanÄ±cÄ± sorusunu yanÄ±tla. Bilgiler yetmezse spekÃ¼lasyon yapma.

Bilgi:
{context}

Soru:
{user_input}

Cevap:
"""

    # Gemini ile yanÄ±t al
    try:
        response = llm.generate_content(prompt)
        return response.text.strip()
    except Exception as e:
        return f"[AI HatasÄ±]: {str(e)}"


# ğŸ›  Bu dosya direkt Ã§alÄ±ÅŸtÄ±rÄ±lÄ±rsa embedding Ã¼ret
if __name__ == "__main__":
    build_vector_store()
    print("âœ… VektÃ¶r veritabanÄ± baÅŸarÄ±yla oluÅŸturuldu.")
